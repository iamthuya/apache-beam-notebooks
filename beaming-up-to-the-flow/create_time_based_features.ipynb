{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "## Beaming up to the Flow\n",
    "\n",
    "This notebook contains a step-by-step guide to create a <b>streaming</b> data pipline that engineers time-based features for a fraud detection system in real-time.\n",
    "\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/ai-platform/notebooks/deploy-notebook?name=create-time-based-features&download_url=https://raw.githubusercontent.com/iamthuya/apache-beam-notebooks/main/beaming-up-to-the-flow/create_time_based_features.ipynb\" target=”_blank”>\n",
    "       <img src=\"https://www.gstatic.com/cloud/images/navigation/vertex-ai.svg\" alt=\"Google Cloud Notebooks\">Open in Cloud Notebook\n",
    "    </a>\n",
    "  </td> \n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/iamthuya/apache-beam-notebooks/blob/main/beaming-up-to-the-flow/create_time_based_features.ipynb\" target=”_blank”>\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/iamthuya/apache-beam-notebooks/blob/main/beaming-up-to-the-flow/create_time_based_features.ipynb\" target=”_blank”>\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO",
    "tags": []
   },
   "source": [
    "### Overview\n",
    "\n",
    "\n",
    "We will first use [Apache Beam](https://beam.apache.org/) to create a data pipeline that works with a batch of data. After that we will change the input source to pulls the streaming data directly from a [Pub/Sub](https://cloud.google.com/pubsub) topic. Once the pipeline is working locally, we will deploy it to the [Dataflow](https://cloud.google.com/dataflow).\n",
    "\n",
    "As for the features, we will be creating\n",
    "- customer spending features (last 15-mins, 30-mins, and 60-mins)\n",
    "- terminal transacting features (last 15-mins, 30-mins, and 60-mins)\n",
    "\n",
    "Note: Before creating the features, an exploratory data analysis (EDA) should be performed on the data to understand the statistics and the corrlations better. We won't be covering the EDA part in this notebook as it is considered out of scope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "459u5__6HXxj"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset is synthesized by using the code from [Machine Learning for Credit Card Fraud Detection - Practical Handbook project from Kaggle](https://github.com/Fraud-Detection-Handbook/fraud-detection-handbook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FE4AVpijHXxj"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In the following notebook, you will learn to:\n",
    "\n",
    "- Use Apache Beam to create data pipelines\n",
    "- Apply Apache Beam's windowing and aggreation functions to create features\n",
    "- Deploy the Apache Beam pipelines to Dataflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMBCHYNyH4T9"
   },
   "source": [
    "## Setting up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ut9wGsbrHXxk"
   },
   "source": [
    "### Enlarge the display\n",
    "\n",
    "First, let's enlarge the jupyter notebook for viewing pleasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nrl8B0QKHXxk"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hhaqa6rwHXxl"
   },
   "source": [
    "### Install additional packages\n",
    "\n",
    "Before we get started, let's install the required packages to execute this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TaJvZvb6HXxm"
   },
   "outputs": [],
   "source": [
    "!pip install -q apache-beam[gcp] pandas --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xeOHogEqHXxm"
   },
   "source": [
    "### Restart runtime\n",
    "\n",
    "Restarting the Jupyter Kernel is necessary to reflect the newly installed packages in current runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nuKkn8UfHXxn"
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True) # automatically restarts kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0JBhoLKIS-c"
   },
   "source": [
    "## Creating data pipelines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf",
    "tags": []
   },
   "source": [
    "### Import libraries\n",
    "\n",
    "Importing all the requied libraries for the later part of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vs7RLpxcHXxn"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "from typing import Tuple, Any, List\n",
    "\n",
    "import apache_beam as beam\n",
    "\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.transforms.combiners import CountCombineFn, MeanCombineFn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Moq4QZKjY4fv",
    "tags": []
   },
   "source": [
    "### Define constant variables\n",
    "\n",
    "Constant variables are defined here to be used in the later part of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q0yOs3P9HXxo"
   },
   "outputs": [],
   "source": [
    "MINUTES_WINDOWS = [15, 30, 60]\n",
    "SECONDS_WINDOWS = [x * 60 for x in sorted(MINUTES_WINDOWS)]\n",
    "WINDOW_SIZE = SECONDS_WINDOWS[-1]  # the largest window\n",
    "WINDOW_PERIOD = 1 * 60  # 1 minute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cB8e49oLHXxo"
   },
   "source": [
    "### Defining auxiliary functions and classes\n",
    "\n",
    "Here we define auxiliary functions and classes for data transformation to be used in the pipelines later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MvMcIqvDHXxo"
   },
   "outputs": [],
   "source": [
    "def to_unix_time(time_str: str, time_format='%Y-%m-%d %H:%M:%S') -> int:\n",
    "    \"\"\"Converts a time string into Unix time.\"\"\"\n",
    "    time_tuple = time.strptime(time_str, time_format)\n",
    "    return int(time.mktime(time_tuple))\n",
    "\n",
    "\n",
    "class PrintElementInfo(beam.DoFn):\n",
    "    \"\"\"Prints an element with its information.\"\"\"\n",
    "    def process(self, element, timestamp=beam.DoFn.TimestampParam, window=beam.DoFn.WindowParam):\n",
    "        print(f\"{element}\\n\")\n",
    "        yield (element,)\n",
    "\n",
    "\n",
    "class AddAddtionalInfo(beam.DoFn):\n",
    "    \"\"\"Add composite key and difference from window end timestamp to element\"\"\"\n",
    "    def process(self, element: Tuple, timestamp=beam.DoFn.TimestampParam, window=beam.DoFn.WindowParam) -> Tuple:\n",
    "        window_end_dt = window.end.to_utc_datetime().strftime(\"%Y%m%d%H%M%S\")\n",
    "        new_element = {\n",
    "            'TX_ID': element['TX_ID'],\n",
    "            'TX_TS': element['TX_TS'],                                                                                                                                                                                                                                      \n",
    "            'CUSTOMER_ID': element['CUSTOMER_ID'],\n",
    "            'TERMINAL_ID': element['TERMINAL_ID'],\n",
    "            'TX_AMOUNT': element['TX_AMOUNT'],\n",
    "            'TX_FRAUD': element['TX_FRAUD'],\n",
    "            'CID_COMPOSITE_KEY': f\"{element['CUSTOMER_ID']}_{window_end_dt}\",\n",
    "            'TID_COMPOSITE_KEY': f\"{element['TERMINAL_ID']}_{window_end_dt}\",\n",
    "            'TS_DIFF': window.end - timestamp\n",
    "        }\n",
    "        return (new_element,)\n",
    "\n",
    "    \n",
    "class Reformat(beam.DoFn):\n",
    "    \"\"\"Reformat the element such that it appears as one dictionary\"\"\"\n",
    "    def process(self, element: Tuple) -> Tuple:\n",
    "        new_records = element[1]['new_records']\n",
    "        aggregated = element[1]['aggregated_customer_id'][0]\n",
    "        \n",
    "        for row in new_records:\n",
    "            new_element = {\n",
    "                'TX_ID': row.TX_ID,\n",
    "                'TX_TS': row.TX_TS,\n",
    "                'CUSTOMER_ID': row.CUSTOMER_ID,\n",
    "                'TERMINAL_ID': row.TERMINAL_ID,\n",
    "                'TX_AMOUNT': row.TX_AMOUNT,\n",
    "                'TX_FRAUD': row.TX_FRAUD,\n",
    "                'CID_NUM_TX_15MIN': aggregated.CID_NUM_TX_15MIN,\n",
    "                'CID_AVG_AMOUNT_15MIN': aggregated.CID_SUM_AMOUNT_15MIN / aggregated.CID_NUM_TX_15MIN,\n",
    "                'CID_NUM_TX_30MIN': aggregated.CID_NUM_TX_30MIN,\n",
    "                'CID_AVG_AMOUNT_30MIN': aggregated.CID_SUM_AMOUNT_30MIN / aggregated.CID_NUM_TX_30MIN,\n",
    "                'CID_NUM_TX_60MIN': aggregated.CID_NUM_TX_60MIN,\n",
    "                'CID_AVG_AMOUNT_60MIN': aggregated.CID_AVG_AMOUNT_60MIN,\n",
    "            }\n",
    "            yield (new_element,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O641mn3IHXxp"
   },
   "source": [
    "## Working with batch data\n",
    "\n",
    "We start by creating a dummy data that ressemble the actual data. The reason for doing this is that Apache Beam works the same for both batch and streaming data and it is easier to debug with batch data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GnaQtWXBHXxp"
   },
   "outputs": [],
   "source": [
    "dummy_data = [\n",
    "    {'TX_ID': '01', 'TX_TS': '2022-08-04 08:36:00', 'CUSTOMER_ID': 'A1', 'TERMINAL_ID': 'T4', 'TX_AMOUNT': 10.00, 'TX_FRAUD': 0},\n",
    "    {'TX_ID': '02', 'TX_TS': '2022-08-04 08:36:30', 'CUSTOMER_ID': 'B2', 'TERMINAL_ID': 'T4', 'TX_AMOUNT': 20.00, 'TX_FRAUD': 0},\n",
    "    {'TX_ID': '03', 'TX_TS': '2022-08-04 08:37:00', 'CUSTOMER_ID': 'A1', 'TERMINAL_ID': 'T4', 'TX_AMOUNT': 10.00, 'TX_FRAUD': 0},\n",
    "    {'TX_ID': '04', 'TX_TS': '2022-08-04 08:37:30', 'CUSTOMER_ID': 'B2', 'TERMINAL_ID': 'T4', 'TX_AMOUNT': 20.00, 'TX_FRAUD': 0},\n",
    "    {'TX_ID': '05', 'TX_TS': '2022-08-04 08:38:00', 'CUSTOMER_ID': 'A1', 'TERMINAL_ID': 'T4', 'TX_AMOUNT': 10.00, 'TX_FRAUD': 0},\n",
    "    {'TX_ID': '06', 'TX_TS': '2022-08-04 08:38:30', 'CUSTOMER_ID': 'B2', 'TERMINAL_ID': 'T4', 'TX_AMOUNT': 20.00, 'TX_FRAUD': 0},\n",
    "    {'TX_ID': '07', 'TX_TS': '2022-08-04 08:39:00', 'CUSTOMER_ID': 'A1', 'TERMINAL_ID': 'T4', 'TX_AMOUNT': 10.00, 'TX_FRAUD': 0},\n",
    "    {'TX_ID': '08', 'TX_TS': '2022-08-04 08:39:30', 'CUSTOMER_ID': 'B2', 'TERMINAL_ID': 'T4', 'TX_AMOUNT': 20.00, 'TX_FRAUD': 0},\n",
    "    {'TX_ID': '09', 'TX_TS': '2022-08-04 08:40:00', 'CUSTOMER_ID': 'A1', 'TERMINAL_ID': 'T4', 'TX_AMOUNT': 10.00, 'TX_FRAUD': 0},\n",
    "    {'TX_ID': '10', 'TX_TS': '2022-08-04 08:40:30', 'CUSTOMER_ID': 'B2', 'TERMINAL_ID': 'T4', 'TX_AMOUNT': 20.00, 'TX_FRAUD': 0},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzRm-tG7HXxp"
   },
   "source": [
    "Here we define pipeline options to pass it to our data pipeline. `DirectRunner` means running locally. Let's check whether loading the dummy data is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4VUBf7yCHXxp"
   },
   "outputs": [],
   "source": [
    "# define pipeline options\n",
    "local_batch_options = PipelineOptions(flags=[], \n",
    "                                      type_check_additional='all',\n",
    "                                      save_main_session=True,\n",
    "                                      runner='DirectRunner')\n",
    "\n",
    "# run the pipeline\n",
    "with beam.Pipeline(options=local_batch_options) as pipeline:\n",
    "    source = (\n",
    "        pipeline\n",
    "        | 'Create dummy data' >> beam.Create(dummy_data)\n",
    "        | 'Print elements' >> beam.ParDo(PrintElementInfo())\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eoDu6scBHXxq"
   },
   "source": [
    "Let's enrich the data by adding timestamps and windows information while slicing the data into defined sliding windows. The results are converted to Row data type so that it can be aggregated easily later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bdT6Lj1cHXxq"
   },
   "outputs": [],
   "source": [
    "with beam.Pipeline(options=local_batch_options) as pipeline:\n",
    "    source = (\n",
    "        pipeline\n",
    "        | 'Create dummy data' >> beam.Create(dummy_data)\n",
    "    )\n",
    "    \n",
    "    enriched_source = (\n",
    "        source\n",
    "        | 'Attach timestamps' >> beam.Map(lambda row: beam.window.TimestampedValue(row, to_unix_time(row['TX_TS'])))\n",
    "        | 'Create sliding window' >> beam.WindowInto(beam.window.SlidingWindows(WINDOW_SIZE, WINDOW_PERIOD, offset=WINDOW_SIZE))\n",
    "        | 'Add window info' >> beam.ParDo(AddAddtionalInfo())\n",
    "        | 'Convert to namedtuple' >> beam.Map(lambda row: beam.Row(**row))\n",
    "        | 'Print elements' >> beam.ParDo(PrintElementInfo())\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to create requied features. We will make use of `GroupBy` and `aggregate_field` functions to perform aggregations. Rows are selected based on the window sizes  defined earlier and the time difference with window end time of each window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "woa3RM1fHXxq"
   },
   "outputs": [],
   "source": [
    "with beam.Pipeline(options=local_batch_options) as pipeline:\n",
    "    source = (\n",
    "        pipeline\n",
    "        | 'Create dummy data' >> beam.Create(dummy_data)\n",
    "    )\n",
    "    \n",
    "    enriched_source = (\n",
    "        source\n",
    "        | 'Attach timestamps' >> beam.Map(lambda row: beam.window.TimestampedValue(row, to_unix_time(row['TX_TS'])))\n",
    "        | 'Create sliding window' >> beam.WindowInto(beam.window.SlidingWindows(WINDOW_SIZE, WINDOW_PERIOD, offset=WINDOW_SIZE))\n",
    "        | 'Add window info' >> beam.ParDo(AddAddtionalInfo())\n",
    "        | 'Convert to namedtuple' >> beam.Map(lambda row: beam.Row(**row))\n",
    "    )\n",
    "    \n",
    "    aggregated_customer_id = (\n",
    "        enriched_source\n",
    "        | 'Group by customer id composite key column' >> beam.GroupBy(CID_COMPOSITE_KEY='CID_COMPOSITE_KEY')\n",
    "            .aggregate_field(lambda row: 1 if row.TS_DIFF <= SECONDS_WINDOWS[0] else 0, sum, 'CID_NUM_TX_15MIN')\n",
    "            .aggregate_field(lambda row: row.TX_AMOUNT if row.TS_DIFF <= SECONDS_WINDOWS[0] else 0, sum,'CID_SUM_AMOUNT_15MIN')\n",
    "            .aggregate_field(lambda row: 1 if row.TS_DIFF <= SECONDS_WINDOWS[1] else 0, sum, 'CID_NUM_TX_30MIN')\n",
    "            .aggregate_field(lambda row: row.TX_AMOUNT if row.TS_DIFF <= SECONDS_WINDOWS[1] else 0, sum, 'CID_SUM_AMOUNT_30MIN')\n",
    "            .aggregate_field('TX_ID', CountCombineFn(), 'CID_NUM_TX_60MIN')\n",
    "            .aggregate_field('TX_AMOUNT', MeanCombineFn(), 'CID_AVG_AMOUNT_60MIN')\n",
    "        | 'Assign key for aggregated results (customer id)' >> beam.WithKeys(lambda row: row.CID_COMPOSITE_KEY)\n",
    "        | 'Print elements' >> beam.ParDo(PrintElementInfo())\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8sU837bSPrdj"
   },
   "source": [
    "Since we are only moving by 1 minute window period, we need to get a slice of those new data within the 1 minute window period and join them with the aggregate features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5fK55T3mHXxq"
   },
   "outputs": [],
   "source": [
    "with beam.Pipeline(options=local_batch_options) as pipeline:\n",
    "    source = (\n",
    "        pipeline\n",
    "        | 'Create dummy data' >> beam.Create(dummy_data)\n",
    "    )\n",
    "    \n",
    "    enriched_source = (\n",
    "        source\n",
    "        | 'Attach timestamps' >> beam.Map(lambda row: beam.window.TimestampedValue(row, to_unix_time(row['TX_TS'])))\n",
    "        | 'Create sliding window' >> beam.WindowInto(beam.window.SlidingWindows(WINDOW_SIZE, WINDOW_PERIOD, offset=WINDOW_SIZE))\n",
    "        | 'Add window info' >> beam.ParDo(AddAddtionalInfo())\n",
    "        | 'Convert to namedtuple' >> beam.Map(lambda row: beam.Row(**row))\n",
    "    )\n",
    "    \n",
    "    aggregated_customer_id = (\n",
    "        enriched_source\n",
    "        | 'Group by customer id composite key column' >> beam.GroupBy(CID_COMPOSITE_KEY='CID_COMPOSITE_KEY')\n",
    "            .aggregate_field(lambda row: 1 if row.TS_DIFF <= SECONDS_WINDOWS[0] else 0, sum, 'CID_NUM_TX_15MIN')\n",
    "            .aggregate_field(lambda row: row.TX_AMOUNT if row.TS_DIFF <= SECONDS_WINDOWS[0] else 0, sum,'CID_SUM_AMOUNT_15MIN')\n",
    "            .aggregate_field(lambda row: 1 if row.TS_DIFF <= SECONDS_WINDOWS[1] else 0, sum, 'CID_NUM_TX_30MIN')\n",
    "            .aggregate_field(lambda row: row.TX_AMOUNT if row.TS_DIFF <= SECONDS_WINDOWS[1] else 0, sum, 'CID_SUM_AMOUNT_30MIN')\n",
    "            .aggregate_field('TX_ID', CountCombineFn(), 'CID_NUM_TX_60MIN')\n",
    "            .aggregate_field('TX_AMOUNT', MeanCombineFn(), 'CID_AVG_AMOUNT_60MIN')\n",
    "        | 'Assign key for aggregated results (customer id)' >> beam.WithKeys(lambda row: row.CID_COMPOSITE_KEY)\n",
    "    )\n",
    "    \n",
    "    new_records = (\n",
    "        enriched_source\n",
    "        | 'Filter only new elements' >> beam.Filter(lambda row: row.TS_DIFF <= WINDOW_PERIOD)\n",
    "        | 'Assign key for new elements' >> beam.WithKeys(lambda row: row.CID_COMPOSITE_KEY)\n",
    "    )\n",
    "\n",
    "    result = (\n",
    "        ({\n",
    "            'new_records': new_records, \n",
    "            'aggregated_customer_id': aggregated_customer_id\n",
    "        })\n",
    "        | 'Merge pcollections (customer id)' >> beam.CoGroupByKey()\n",
    "        | 'Filter empty rows (customer id)' >> beam.Filter(lambda row: len(row[1]['new_records']) > 0)\n",
    "        | 'Reformat elements (customer id)' >> beam.ParDo(Reformat())\n",
    "        | 'Print elements' >> beam.ParDo(PrintElementInfo())\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YrzD-rFOHXxq"
   },
   "source": [
    "## Setting up Google Cloud project\n",
    "\n",
    "- Step 1: Go to [Google Cloud Console](https://console.cloud.google.com/projectcreate) and create a project. Select the created project.\n",
    "\n",
    "- Step 2: Activate [Cloud Shell](https://console.cloud.google.com/?cloudshell=true) and enter the following commands: (Authorize it if prompted)\n",
    "```\n",
    "gcloud services enable pubsub dataflow\n",
    "gcloud pubsub subscriptions create \"ff-txlabels-sub\" --topic=\"ff-txlabels\" --topic-project=\"cymbal-fraudfinder\"\n",
    "export PROJECT_ID=$(gcloud config get-value project)\n",
    "gsutil mb -c STANDARD -l US gs://${PROJECT_ID}\n",
    "```\n",
    "\n",
    "- Step 3: Create a [Service Account Key](https://cloud.google.com/iam/docs/creating-managing-service-account-keys). (Choose json format and download it).\n",
    "\n",
    "- Step 4: Go to the [IAM Admin](https://console.cloud.google.com/iam-admin). Click on `ADD`. Put your service account email address as `New Principals` and select `Owner` as role. Click `Save`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtMo2nvEHXxr"
   },
   "source": [
    "### Set Google Application Credentials\n",
    "\n",
    "If the steps from previous cell are successful, you should have a json file. Assign it as `GOOGLE_APPLICATION_CREDENTIALS` variable and you are all set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cCCh_1A6HXxr"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"path-to-your-service-account-key.json\" # update this value with yours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24w0lN35HXxr"
   },
   "source": [
    "### Define Google Cloud variables\n",
    "\n",
    "Google Cloud variables are defined here to be used in the later part of this notebook. You can find these from the json file that you just downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0mNXV9FUHXxr"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = 'your-project-id'  # update this value with yours\n",
    "REGION = 'us-central1'  # update this value with yours\n",
    "SUBSCRIPTION_NAME = \"ff-txlabels-sub\"\n",
    "SUBSCRIPTION_PATH = f\"projects/{PROJECT_ID}/subscriptions/{SUBSCRIPTION_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rLHKnMgdIhaW"
   },
   "source": [
    "## Working with streaming data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UoEH6YfHXxr"
   },
   "source": [
    "### Reading sample messages from the Pub/Sub topic\n",
    "\n",
    "Pulling sample messages from the pub/sub topic to verfiy whether the upstream is working as intended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LWtmzzHYHXxr",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "from google.api_core import retry\n",
    "from google.cloud import pubsub_v1\n",
    "\n",
    "def read_from_sub(project_id, subscription_name, messages=10):\n",
    "    subscriber = pubsub_v1.SubscriberClient()\n",
    "    subscription_path = subscriber.subscription_path(project_id, subscription_name)\n",
    "\n",
    "    # Wrap the subscriber in a 'with' block to automatically call close() to\n",
    "    # close the underlying gRPC channel when done.\n",
    "    with subscriber:\n",
    "        # The subscriber pulls a specific number of messages. The actual\n",
    "        # number of messages pulled may be smaller than max_messages.\n",
    "        response = subscriber.pull(\n",
    "            subscription=subscription_path,\n",
    "            max_messages=messages,\n",
    "            retry=retry.Retry(deadline=300),\n",
    "        )\n",
    "\n",
    "        if len(response.received_messages) == 0:\n",
    "            print(\"no messages\")\n",
    "            return\n",
    "\n",
    "        ack_ids = []\n",
    "        msg_data = []\n",
    "        for received_message in response.received_messages:\n",
    "            msg = ast.literal_eval(received_message.message.data.decode('utf-8'))\n",
    "            msg_data.append(msg)\n",
    "            ack_ids.append(received_message.ack_id)\n",
    "\n",
    "        # Acknowledges the received messages so they will not be sent again.\n",
    "        subscriber.acknowledge(subscription=subscription_path, ack_ids=ack_ids)\n",
    "\n",
    "        print(\n",
    "            f\"Received and acknowledged {len(response.received_messages)} messages from {subscription_path}.\"\n",
    "        )\n",
    "\n",
    "        return msg_data\n",
    "\n",
    "\n",
    "messages_tx = read_from_sub(project_id=PROJECT_ID,\n",
    "                            subscription_name=SUBSCRIPTION_NAME,\n",
    "                            messages=5)\n",
    "\n",
    "messages_tx[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_W7drJAHXxr"
   },
   "source": [
    "### Running streaming pipeline using DirectRunner\n",
    "\n",
    "Here we just need to add one more option to the PipelineOptions (`streaming=True`) and it will work seamlessly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CNh_QmnZHXxr"
   },
   "outputs": [],
   "source": [
    "local_stream_options = PipelineOptions(flags=[], \n",
    "                                       type_check_additional='all',\n",
    "                                       save_main_session=True,\n",
    "                                       runner='DirectRunner', \n",
    "                                       streaming=True)\n",
    "\n",
    "with beam.Pipeline(options=local_stream_options) as pipeline:\n",
    "    source = (\n",
    "        pipeline\n",
    "        | 'Read from Pub/Sub' >> beam.io.ReadFromPubSub(subscription=SUBSCRIPTION_PATH)\n",
    "        | 'Decode byte array to json dict' >> beam.Map(lambda row: json.loads(row.decode('utf-8')))\n",
    "    )\n",
    "    \n",
    "    enriched_source = (\n",
    "        source\n",
    "        | 'Attach timestamps' >> beam.Map(lambda row: beam.window.TimestampedValue(row, to_unix_time(row['TX_TS'])))\n",
    "        | 'Create sliding window' >> beam.WindowInto(beam.window.SlidingWindows(WINDOW_SIZE, WINDOW_PERIOD, offset=WINDOW_SIZE))\n",
    "        | 'Add window info' >> beam.ParDo(AddAddtionalInfo())\n",
    "        | 'Convert to namedtuple' >> beam.Map(lambda row: beam.Row(**row))\n",
    "    )\n",
    "    \n",
    "    aggregated_customer_id = (\n",
    "        enriched_source\n",
    "        | 'Group by customer id composite key column' >> beam.GroupBy(CID_COMPOSITE_KEY='CID_COMPOSITE_KEY')\n",
    "            .aggregate_field(lambda row: 1 if row.TS_DIFF <= SECONDS_WINDOWS[0] else 0, sum, 'CID_NUM_TX_15MIN')\n",
    "            .aggregate_field(lambda row: row.TX_AMOUNT if row.TS_DIFF <= SECONDS_WINDOWS[0] else 0, sum,'CID_SUM_AMOUNT_15MIN')\n",
    "            .aggregate_field(lambda row: 1 if row.TS_DIFF <= SECONDS_WINDOWS[1] else 0, sum, 'CID_NUM_TX_30MIN')\n",
    "            .aggregate_field(lambda row: row.TX_AMOUNT if row.TS_DIFF <= SECONDS_WINDOWS[1] else 0, sum, 'CID_SUM_AMOUNT_30MIN')\n",
    "            .aggregate_field('TX_ID', CountCombineFn(), 'CID_NUM_TX_60MIN')\n",
    "            .aggregate_field('TX_AMOUNT', MeanCombineFn(), 'CID_AVG_AMOUNT_60MIN')\n",
    "        | 'Assign key for aggregated results (customer id)' >> beam.WithKeys(lambda row: row.CID_COMPOSITE_KEY)\n",
    "\n",
    "    )\n",
    "    \n",
    "    new_records = (\n",
    "        enriched_source\n",
    "        | 'Filter only new elements' >> beam.Filter(lambda row: row.TS_DIFF <= WINDOW_PERIOD)\n",
    "        | 'Assign key for new elements' >> beam.WithKeys(lambda row: row.CID_COMPOSITE_KEY)\n",
    "    )\n",
    "\n",
    "    result = (\n",
    "        ({\n",
    "            'new_records': new_records, \n",
    "            'aggregated_customer_id': aggregated_customer_id\n",
    "        })\n",
    "        | 'Merge pcollections (customer id)' >> beam.CoGroupByKey()\n",
    "        | 'Filter empty rows (customer id)' >> beam.Filter(lambda row: len(row[1]['new_records']) > 0)\n",
    "        | 'Reformat elements (customer id)' >> beam.ParDo(Reformat())\n",
    "        | 'Print elements' >> beam.ParDo(PrintElementInfo())\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hHMEYbL7HXxs"
   },
   "source": [
    "### Running streaming pipeline using DataflowRunner\n",
    "\n",
    "Deploying the pipeline to Dataflow is quite straightforward. We just need to change the `runner='DirectRunner'` to `runner='DataflowRunner'` and add Google Cloud specific options such as `project`, `region`, and `temp_location`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rR80xIBnHXxs"
   },
   "outputs": [],
   "source": [
    "dataflow_stream_options = PipelineOptions(flags=[],\n",
    "                                          type_check_additional='all',\n",
    "                                          save_main_session=True,\n",
    "                                          runner='DataflowRunner',\n",
    "                                          streaming=True,\n",
    "                                          project=PROJECT_ID,\n",
    "                                          region=REGION,\n",
    "                                          temp_location=f\"gs://{PROJECT_ID}/tmp\")\n",
    "\n",
    "with beam.Pipeline(options=dataflow_stream_options) as pipeline:\n",
    "    source = (\n",
    "        pipeline\n",
    "        | 'Read from Pub/Sub' >> beam.io.ReadFromPubSub(subscription=SUBSCRIPTION_PATH)\n",
    "        | 'Decode byte array to json dict' >> beam.Map(lambda row: json.loads(row.decode('utf-8')))\n",
    "    )\n",
    "    \n",
    "    enriched_source = (\n",
    "        source\n",
    "        | 'Attach timestamps' >> beam.Map(lambda row: beam.window.TimestampedValue(row, to_unix_time(row['TX_TS'])))\n",
    "        | 'Create sliding window' >> beam.WindowInto(beam.window.SlidingWindows(WINDOW_SIZE, WINDOW_PERIOD, offset=WINDOW_SIZE))\n",
    "        | 'Add window info' >> beam.ParDo(AddAddtionalInfo())\n",
    "        | 'Convert to namedtuple' >> beam.Map(lambda row: beam.Row(**row))\n",
    "    )\n",
    "    \n",
    "    aggregated_customer_id = (\n",
    "        enriched_source\n",
    "        | 'Group by customer id composite key column' >> beam.GroupBy(CID_COMPOSITE_KEY='CID_COMPOSITE_KEY')\n",
    "            .aggregate_field(lambda row: 1 if row.TS_DIFF <= SECONDS_WINDOWS[0] else 0, sum, 'CID_NUM_TX_15MIN')\n",
    "            .aggregate_field(lambda row: row.TX_AMOUNT if row.TS_DIFF <= SECONDS_WINDOWS[0] else 0, sum,'CID_SUM_AMOUNT_15MIN')\n",
    "            .aggregate_field(lambda row: 1 if row.TS_DIFF <= SECONDS_WINDOWS[1] else 0, sum, 'CID_NUM_TX_30MIN')\n",
    "            .aggregate_field(lambda row: row.TX_AMOUNT if row.TS_DIFF <= SECONDS_WINDOWS[1] else 0, sum, 'CID_SUM_AMOUNT_30MIN')\n",
    "            .aggregate_field('TX_ID', CountCombineFn(), 'CID_NUM_TX_60MIN')\n",
    "            .aggregate_field('TX_AMOUNT', MeanCombineFn(), 'CID_AVG_AMOUNT_60MIN')\n",
    "        | 'Assign key for aggregated results (customer id)' >> beam.WithKeys(lambda row: row.CID_COMPOSITE_KEY)\n",
    "\n",
    "    )\n",
    "    \n",
    "    new_records = (\n",
    "        enriched_source\n",
    "        | 'Filter only new elements' >> beam.Filter(lambda row: row.TS_DIFF <= WINDOW_PERIOD)\n",
    "        | 'Assign key for new elements' >> beam.WithKeys(lambda row: row.CID_COMPOSITE_KEY)\n",
    "    )\n",
    "\n",
    "    result = (\n",
    "        ({\n",
    "            'new_records': new_records, \n",
    "            'aggregated_customer_id': aggregated_customer_id\n",
    "        })\n",
    "        | 'Merge pcollections (customer id)' >> beam.CoGroupByKey()\n",
    "        | 'Filter empty rows (customer id)' >> beam.Filter(lambda row: len(row[1]['new_records']) > 0)\n",
    "        | 'Reformat elements (customer id)' >> beam.ParDo(Reformat())\n",
    "        | 'Print elements' >> beam.ParDo(PrintElementInfo())\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hk9_otUbJ8U4"
   },
   "source": [
    "Congrats! Now the job should be running on [Dataflow](https://console.cloud.google.com/dataflow/jobs)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m95"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
